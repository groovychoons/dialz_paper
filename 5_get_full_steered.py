import datetime
import pandas as pd
from transformers import AutoTokenizer
from datasets import load_dataset
from dialz import Dataset, ControlModel, ControlVector


# Map model names to short names (if needed)
MODEL_SHORT_NAMES = {
    "qwen": "Qwen/Qwen2.5-7B-Instruct",
    "llama": "meta-llama/Llama-3.1-8B-Instruct",
    "mistral": "mistralai/Mistral-7B-Instruct-v0.1",
}

# Timestamp for output file naming
timestamp = datetime.datetime.now().strftime("%Y%m%d-%H%M")

# Read the best trials CSV (update path as needed)
BEST_TRIALS_PATH = "./results/best_trials.csv"
best_trials_df = pd.read_csv(BEST_TRIALS_PATH)

# Load your BBQ data subsets
gender_df = pd.read_json("./data/bbq/BBQ_Gender_identity.jsonl", lines=True)
race_df = pd.read_json("./data/bbq/BBQ_Race_ethnicity.jsonl", lines=True)
racexgender_df = pd.read_json("./data/bbq/BBQ_Race_x_gender.jsonl", lines=True)

# Weâ€™ll evaluate on these data splits
data_files = [
    ("gender", gender_df),
    ("race", race_df),
    ("racexgender", racexgender_df),
]

# Load MMLU
mmlu = load_dataset("cais/mmlu", "all", split="test")
mmlu_df = pd.DataFrame(mmlu)

################################################################################
# 2. Helper Functions
################################################################################

def generate_answer(model, vector, coeff, tokenizer, context, question, answers, task,
                    max_new_tokens=20, repetition_penalty=1.1):
    """
    Given a model and a steering vector, generate a single-letter answer for
    either BBQ or MMLU. 
    """
    if task == "bbq":
        # For 3 answer choices: a, b, c
        input_text = (
            f"Answer with only a single letter.\n\n"
            f"The following are multiple choice questions (with answers):\n"
            f"{context} {question}\n"
            f"(a) {answers[0]}\n"
            f"(b) {answers[1]}\n"
            f"(c) {answers[2]}\n"
        )
    elif task == "mmlu":
        # For 4 answer choices: a, b, c, d
        input_text = (
            f"Answer with only a single letter.\n\n"
            f"The following are multiple choice questions (with answers):\n"
            f"{question}\n"
            f"(a) {answers[0]}\n"
            f"(b) {answers[1]}\n"
            f"(c) {answers[2]}\n"
            f"(d) {answers[3]}\n"
        )
    else:
        raise ValueError("Task must be either 'bbq' or 'mmlu'.")

    messages = [{"role": "user", "content": input_text}]

    chat_input = tokenizer.apply_chat_template(
        messages,
        tokenize=False,
        add_generation_prompt=True,
        return_tensors="pt"
    )
    input_ids = tokenizer(chat_input, return_tensors="pt", add_special_tokens=False).to(model.device)

    # Set up generation hyperparams
    gen_kwargs = {
        "pad_token_id": tokenizer.pad_token_id,
        "do_sample": False,  # Greedy decoding
        "max_new_tokens": max_new_tokens,
        "repetition_penalty": repetition_penalty,
        "return_dict_in_generate": True,
    }

    # Steering
    model.set_control(vector, coeff)

    generated_outputs = model.generate(**input_ids, **gen_kwargs)
    new_tokens = generated_outputs.sequences[0, input_ids["input_ids"].size(1):]
    answer_text = tokenizer.decode(new_tokens, skip_special_tokens=True).strip()

    return answer_text

def get_prediction_with_vector(model, vector, coeff, tokenizer,
                               context, question, answers, task):
    """
    Generates the model's answer and converts it to an integer index 
    for either BBQ or MMLU tasks.
    """
    generated_answer = generate_answer(
        model, vector, coeff, tokenizer, context, question, answers, task
    )
    generated_answer = generated_answer.lower()

    if task == "bbq":
        answer_letters = ["a", "b", "c"]
        expanded_letters = ["(a)", "(b)", "(c)"]
    else:  # mmlu
        answer_letters = ["a", "b", "c", "d"]
        expanded_letters = ["(a)", "(b)", "(c)", "(d)"]

    # 1) Try single-letter matching
    predicted_label = -1
    for i, letter in enumerate(answer_letters):
        if letter in generated_answer[:2]:  # e.g. "a" in "a" or "a)" or "a."
            predicted_label = i
            break

    # 2) Try expanded like "(a)", "(b)"
    if predicted_label == -1:
        for i, letter in enumerate(expanded_letters):
            if letter in generated_answer:
                predicted_label = i
                break

    # 3) For BBQ, fallback to searching for full text of answer
    if predicted_label == -1 and task == "bbq":
        for i, ans_text in enumerate(answers):
            if ans_text.lower() in generated_answer:
                predicted_label = i
                break

    return predicted_label

def evaluate_on_bbq(model, vector, coeff, tokenizer, bbq_df):
    """
    Evaluate accuracy on a given BBQ subset.
    """
    total = len(bbq_df)
    correct = 0
    for idx, row in bbq_df.iterrows():
        if idx % 1000 == 0:
            print(f"Processing row {idx}/{total} at time: {datetime.datetime.now()}")
        context = row["context"]
        question = row["question"]
        answers = [row["ans0"], row["ans1"], row["ans2"]]
        label = row["label"]
        pred = get_prediction_with_vector(
            model, vector, coeff, tokenizer,
            context=context, question=question,
            answers=answers, task="bbq"
        )
        if pred == label:
            correct += 1
    return correct / total if total > 0 else 0.0

def evaluate_on_mmlu(model, vector, coeff, tokenizer, mmlu_df):
    """
    Evaluate accuracy on MMLU test set.
    """
    total = len(mmlu_df)
    correct = 0
    for idx, row in mmlu_df.iterrows():
        if idx % 1000 == 0:
            print(f"Processing row {idx}/{total} at time: {datetime.datetime.now()}")
        question = row["question"]
        answers = row["choices"]
        label = row["answer"]  # 0..3
        pred = get_prediction_with_vector(
            model, vector, coeff, tokenizer,
            context="", question=question,
            answers=answers, task="mmlu"
        )
        if pred == label:
            correct += 1
    return correct / total if total > 0 else 0.0

################################################################################
# 3. Main: Iterate over best trials and evaluate
################################################################################
results = []

# Decide whether or not we are merging
merged = True  # or False

if merged:
    # Group the best_trials_df by model
    grouped_df = best_trials_df.groupby("model")

    for model_short, group in grouped_df:
        # We assume the group has exactly one race row and one gender row
        race_row = group[group["axis"] == "race"].iloc[0]
        gender_row = group[group["axis"] == "gender"].iloc[0]

        # Unpack parameters you want to keep; for simplicity, we take them from the race row.
        model_name    = MODEL_SHORT_NAMES.get(model_short)
        start_layer   = int(race_row["start_layer"])
        end_layer     = int(race_row["end_layer"])
        # Or choose from the gender row or do something fancy like min/max if needed.

        # We'll compute an average coefficient from race_row and gender_row:
        merged_coeff = 0.5 * (float(race_row["coeff"]) + float(gender_row["coeff"]))

        print(f"\n=== Merged evaluation for model: {model_name} ===")
        print(f"Using mean coeff: {merged_coeff}")

        # 1) Create and train the race vector
        race_dataset = Dataset.create_dataset(
            model_name   = model_name,
            items        = [x.strip() for x in race_row["items"].split(",")],
            prompt_type  = race_row["prompt_type"],
            num_sents    = int(race_row["num_sents"]),
            system_role  = race_row["system_prompt"]
        )

        race_layers      = list(range(start_layer, end_layer, -1))
        race_model       = ControlModel(model_name, race_layers)
        race_tokenizer   = AutoTokenizer.from_pretrained(model_name)
        race_tokenizer.pad_token_id = race_tokenizer.eos_token_id
        race_vector      = ControlVector.train(race_model, race_dataset)

        # 2) Create and train the gender vector
        gender_dataset = Dataset.create_dataset(
            model_name   = model_name,
            items        = [x.strip() for x in gender_row["items"].split(",")],
            prompt_type  = gender_row["prompt_type"],
            num_sents    = int(gender_row["num_sents"]),
            system_role  = gender_row["system_prompt"]
        )

        gender_start_layer   = int(gender_row["start_layer"])
        gender_end_layer     = int(gender_row["end_layer"])

        gender_layers    = list(range(gender_start_layer, gender_end_layer, -1))
        gender_model     = ControlModel(model_name, gender_layers)
        gender_tokenizer = AutoTokenizer.from_pretrained(model_name)
        gender_tokenizer.pad_token_id = gender_tokenizer.eos_token_id
        gender_vector    = ControlVector.train(gender_model, gender_dataset)

        # 3) Merge the two vectors by taking the mean
        merged_vector = (race_vector + gender_vector) / 2.0

        # 4) Evaluate on each dataset with the Merged Vector
        #    Re-use your existing evaluate_on_bbq / evaluate_on_mmlu calls
        bbq_race_acc        = evaluate_on_bbq(race_model,   merged_vector, merged_coeff, race_tokenizer, race_df)
        bbq_gender_acc      = evaluate_on_bbq(race_model,   merged_vector, merged_coeff, race_tokenizer, gender_df)
        bbq_racexgender_acc = evaluate_on_bbq(race_model,   merged_vector, merged_coeff, race_tokenizer, racexgender_df)
        mmlu_acc            = evaluate_on_mmlu(race_model,  merged_vector, merged_coeff, race_tokenizer, mmlu_df)

        # 5) Collect results
        results.append({
            "model":               model_name,
            "axis":                "merged",
            "prompt_type":         f"merged-{race_row['prompt_type']}/{gender_row['prompt_type']}",
            "num_sents":           f"merged-{race_row['num_sents']}/{gender_row['num_sents']}",
            "items":               f"{race_row['items']} + {gender_row['items']}",
            "system_prompt":       f"{race_row['system_prompt']} + {gender_row['system_prompt']}",
            "coeff":               merged_coeff,
            "start_layer":         start_layer,
            "end_layer":           end_layer,
            "bbq_race_acc":        bbq_race_acc,
            "bbq_gender_acc":      bbq_gender_acc,
            "bbq_racexgender_acc": bbq_racexgender_acc,
            "mmlu_acc":            mmlu_acc,
        })

else:
    # Original loop (if you just want to benchmark each single best trial)
    for _, trial_row in best_trials_df.iterrows():
        model_short     = trial_row["model"]
        model_name      = MODEL_SHORT_NAMES.get(model_short)

        axis            = trial_row["axis"]
        prompt_type     = trial_row["prompt_type"]
        num_sents       = int(trial_row["num_sents"])
        items_str       = trial_row["items"]
        system_prompt   = trial_row["system_prompt"]
        coeff           = float(trial_row["coeff"])
        start_layer     = int(trial_row["start_layer"])
        end_layer       = int(trial_row["end_layer"])

        items_list = [x.strip() for x in items_str.split(",")]

        print(f"\n=== Evaluating Best Trial ===")
        print(f"Model: {model_name}, Axis: {axis}, prompt_type: {prompt_type}, "
              f"num_sents: {num_sents}, items: {items_list}, system_prompt: {system_prompt}, "
              f"coeff: {coeff}, start_layer: {start_layer}, end_layer: {end_layer}")

        # Create dataset
        dataset = Dataset.create_dataset(
            model_name   = model_name,
            items        = items_list,
            prompt_type  = prompt_type,
            num_sents    = num_sents,
            system_role  = system_prompt
        )

        chosen_layer_ids = list(range(start_layer, end_layer, -1))

        # Load model
        model = ControlModel(model_name, chosen_layer_ids)
        tokenizer = AutoTokenizer.from_pretrained(model_name)
        tokenizer.pad_token_id = tokenizer.eos_token_id

        # Train vector
        vector = ControlVector.train(model, dataset)

        # Evaluate
        bbq_race_acc        = evaluate_on_bbq(model, vector, coeff, tokenizer, race_df)
        bbq_gender_acc      = evaluate_on_bbq(model, vector, coeff, tokenizer, gender_df)
        bbq_racexgender_acc = evaluate_on_bbq(model, vector, coeff, tokenizer, racexgender_df)
        mmlu_acc            = evaluate_on_mmlu(model, vector, coeff, tokenizer, mmlu_df)

        results.append({
            "model":               model_name,
            "axis":                axis,
            "prompt_type":         prompt_type,
            "num_sents":           num_sents,
            "items":               items_str,
            "system_prompt":       system_prompt,
            "coeff":               coeff,
            "start_layer":         start_layer,
            "end_layer":           end_layer,
            "bbq_race_acc":        bbq_race_acc,
            "bbq_gender_acc":      bbq_gender_acc,
            "bbq_racexgender_acc": bbq_racexgender_acc,
            "mmlu_acc":            mmlu_acc,
        })

################################################################################
# 4. Write results to CSV
################################################################################

results_df = pd.DataFrame(results)
output_csv = f"./logs/{timestamp}_best_trials_evaluation.csv"
results_df.to_csv(output_csv, index=False)

print(f"\nDone! Results written to {output_csv}")
